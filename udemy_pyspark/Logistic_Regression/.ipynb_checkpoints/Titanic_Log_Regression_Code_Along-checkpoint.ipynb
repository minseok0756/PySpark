{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Code Along\n",
    "This is a code along of the famous titanic dataset, its always nice to start off with this dataset because it is an example you will find across pretty much every data analysis language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('myproj').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('../titanic.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 컬럼만 가져옴\n",
    "my_cols = data.select(['Survived',\n",
    " 'Pclass',\n",
    " 'Sex',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',\n",
    " 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값은 삭제했음\n",
    "my_final_data = my_cols.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Categorical Columns\n",
    "\n",
    "Let's break this down into multiple steps to make it all clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 단계를 거쳐 원-핫 인코딩한다.\n",
    "# 1. 문자열을 숫자로\n",
    "# 2, 바뀐 숫자들을 원-핫 인코딩\n",
    "# 결과는 벡터 형식으로 출력된다.\n",
    "gender_indexer = StringIndexer(inputCol='Sex',outputCol='SexIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex',outputCol='SexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embark_indexer = StringIndexer(inputCol='Embarked',outputCol='EmbarkIndex')\n",
    "embark_encoder = OneHotEncoder(inputCol='EmbarkIndex',outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pclass',\n",
    " 'SexVec',\n",
    " 'Age',\n",
    " 'SibSp',\n",
    " 'Parch',\n",
    " 'Fare',\n",
    " 'EmbarkVec'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines \n",
    "\n",
    "Let's see an example of how to use pipelines (we'll get a lot more practice with these later!)\n",
    "\n",
    "각 작업의 진행을 위해 단계를 생성하는 역할을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_titanic = LogisticRegression(featuresCol='features',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인은 작업에 필요한 요소를 모두 포함하는 'stages'라는 인자를 갖는다.\n",
    "# (인자에는 앞에서 만들어 놓은 객체를 저장한 변수를 리스트에 나열한다.)\n",
    "# 변환을 위해 단계를 설계하고 집합시킨 다음 모델에게 넘겨주는 것이라 볼 수 있다.\n",
    "# (리스트로 나열하는 것이 단계를 설계하는 것으로 보이고, 집합시키고 모델에\n",
    "# 넘겨주는 것은 파이프라인 객체가 하는 것으로 보인다.)\n",
    "pipeline = Pipeline(stages=[gender_indexer,embark_indexer,\n",
    "                           gender_encoder,embark_encoder,\n",
    "                           assembler,log_reg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_titanic_data, test_titanic_data = my_final_data.randomSplit([0.7,.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 객체의 장점은 일반적인 모델처럼 사용할 수 있다는 것이다.\n",
    "fit_model = pipeline.fit(train_titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터를 변환\n",
    "# 결과물을 테스트 데이터 셋에서 시험해보는거라함\n",
    "# (이전까지는 fit -> evaluate 였고, 배포단계에서 transform을 사용했는데.. 헷갈린다.)\n",
    "# (evaluate은 LinearRegressionSummary객체를 반환한다. 속성으로 평가지표를 갖는다.)\n",
    "# (accuracy, weightedPrecision, weightedRecall, ... )\n",
    "# (여태 사용한 predictions도 있는데 Dataframe outputted by the model's transform method을 의미한다. )\n",
    "results = fit_model.transform(test_titanic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+----+-----+-----+--------+--------+--------+-----------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|Survived|Pclass| Sex| Age|SibSp|Parch|    Fare|Embarked|SexIndex|EmbarkIndex|       SexVec|    EmbarkVec|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+------+----+----+-----+-----+--------+--------+--------+-----------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "|       0|     1|male|24.0|    0|    0|    79.2|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,24.0,0.0...|[-0.8944258156564...|[0.29019733954019...|       1.0|\n",
      "|       0|     1|male|29.0|    0|    0|    30.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,29.0,0.0...|[-0.3476613003529...|[0.41394966430416...|       1.0|\n",
      "|       0|     1|male|31.0|    0|    0| 50.4958|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,31.0,0.0...|[-0.2677832911841...|[0.43345137454243...|       1.0|\n",
      "|       0|     1|male|31.0|    1|    0|    52.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,31.0,1.0...|[0.13602991198327...|[0.53395513493081...|       0.0|\n",
      "|       0|     1|male|33.0|    0|    0|     5.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,33.0,0.0...|[-0.1327348632764...|[0.46686491922440...|       1.0|\n",
      "|       0|     1|male|36.0|    0|    0|  40.125|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,36.0,0.0...|[-0.2796805889649...|[0.43053208567033...|       1.0|\n",
      "|       0|     1|male|36.0|    1|    0|   78.85|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,36.0,1.0...|[0.35611508952838...|[0.58809968254764...|       0.0|\n",
      "|       0|     1|male|37.0|    0|    1|    29.7|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,37.0,0.0...|[-0.0095759130001...|[0.49760604004340...|       1.0|\n",
      "|       0|     1|male|40.0|    0|    0|     0.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|(8,[0,1,2,6],[1.0...|[0.21099055537648...|[0.55255282543257...|       0.0|\n",
      "|       0|     1|male|40.0|    0|    0| 27.7208|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,40.0,0.0...|[-0.0752845157582...|[0.48118775549247...|       1.0|\n",
      "|       0|     1|male|42.0|    1|    0|    52.0|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,42.0,1.0...|[0.66960111256609...|[0.66141383588012...|       0.0|\n",
      "|       0|     1|male|45.0|    0|    0|   26.55|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,45.0,0.0...|[0.43132653947305...|[0.60619038928060...|       0.0|\n",
      "|       0|     1|male|46.0|    0|    0|    79.2|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,46.0,0.0...|[0.17271658550920...|[0.54307212579160...|       0.0|\n",
      "|       0|     1|male|46.0|    1|    0|  61.175|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,46.0,1.0...|[0.85595650332146...|[0.70181516031067...|       0.0|\n",
      "|       0|     1|male|47.0|    0|    0| 25.5875|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,47.0,0.0...|[0.52914415605284...|[0.62928347784360...|       0.0|\n",
      "|       0|     1|male|47.0|    0|    0| 34.0208|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,47.0,0.0...|[0.52209373308459...|[0.62763722065622...|       0.0|\n",
      "|       0|     1|male|49.0|    1|    1|110.8833|       C|     0.0|        1.0|(1,[0],[1.0])|(2,[1],[1.0])|[1.0,1.0,49.0,1.0...|[0.90970149588073...|[0.712939075693,0...|       0.0|\n",
      "|       0|     1|male|54.0|    0|    0| 51.8625|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,54.0,0.0...|[0.84672299171544...|[0.69987926370075...|       0.0|\n",
      "|       0|     1|male|55.0|    0|    0|    30.5|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,55.0,0.0...|[0.91308898101464...|[0.71363184769054...|       0.0|\n",
      "|       0|     1|male|61.0|    0|    0| 32.3208|       S|     0.0|        0.0|(1,[0],[1.0])|(2,[0],[1.0])|[1.0,1.0,61.0,0.0...|[1.20260558913315...|[0.76898797897657...|       0.0|\n",
      "+--------+------+----+----+-----+-----+--------+--------+--------+-----------+-------------+-------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()\n",
    "# test_titanic_data에 파이프라인이 모두 적용되고 예측값까지 추가됨\n",
    "# 그냥 이걸 transform이 하는일이라고 생각하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.show()에서 prediction 컬럼이 추가된 것을 보았다.\n",
    "# 따라서 rawPredictionCol에 'prediction'을 넣었다.\n",
    "# transform하면 항상 prediction 컬럼이 생성된다고 함\n",
    "\n",
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = my_eval.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.790673324742268"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
